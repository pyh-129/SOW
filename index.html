<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SOWing information: Cultivating Contextual Coherence with MLLMs in Image Generation">
  <meta name="keywords" content="Diffusion models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SOWing information: Cultivating Contextual Coherence with MLLMs in Image Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SOWing information: Cultivating Contextual Coherence with MLLMs in Image Generation.</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Yuhan Pei*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Ruoyu Wang*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Yongqi Yang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Ye Zhu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Olga Russakovsky</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Yu Wu</a><sup>1</sup>,
            </span>
            <!-- <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Wuhan University</span>
            <span class="author-block"><sup>2</sup>Princeton University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
 -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Originating from the diffusion phenomenon in physics that describes particle movement, the diffusion generative models inherit the characteristics of a random walk in the data space along the denoising trajectory. 
            However, intrinsic mutual interference among image regions contradicts the need for practical downstream application scenarios where the preservation of low-level pixel information is desired (e.g., customization tasks like personalized generation and inpainting based on a user-provided single image).
            In this work, we investigate the diffusion (physics) in diffusion (machine learning) properties and introduce Cyclic One-Way Diffusion (COW), which constrains chaotic, bidirectional information diffusion into a streamlined unidirectional flow.
            This ensures precise information transfer while minimizing disruptive interference, thus providing a versatile and training-free framework that can be adaptable to a wide range of customization scenarios.
            Building on this, we further propose Selective One-Way Diffusion (SOW), which leverages attention mechanisms and Multimodal Large Language Models (MLLMs) to dynamically regulate the direction and intensity of diffusion based on contextual relationships within the image.
            Our method reframes disordered diffusion as a powerful tool for context-driven, coherent image generation, significantly improving both fidelity and condition consistency. Extensive experiments demonstrate the untapped potential of controlled information diffusion, offering a path to more adaptive and intelligent generative models in a learning-free manner.  
          </p>
        </div>
        <div class="content has-text-justified">
          <img class="columns is-centered has-text-centered" src="./static/images/compare_newa1_00.png" alt="Teaser" width="100%"
               style="margin:0 auto">
          <br>
          <figcaption>
              <p style="text-align: left;">
                  <font color="061E61">
                      <b>Figure 1:</b> Comparison with existing SOTA methods for maintaining the fidelity of text and visual conditions in different application scenarios. We consistently achieve superior fidelity to both text and visual conditions in all three settings. In contrast, other learning-based approaches struggle to attain the same level of performance in diverse scenarios. 
                  </font>
              </p>
          </figcaption>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
          <h2 class="title is-2">Method</h2>
          <br>
      </div>

      <!-- Architecture -->
      <div class="columns is-centered">
          <div class="column is-full-width">
              <h4 class="title is-3">• Framework</h4>

              <div class="content has-text-justified">
                  <img class="columns is-centered has-text-centered" src="./static/images/pipeline_v_00.png" alt="Teaser" width="95%"
                       style="margin:0 auto">
                  <br>
                  <figcaption>
                      <p style="text-align: center; color: #061E61;">
                          <b>Figure 2:</b> The pipeline of our proposed SOW method. Initially, given the visual condition and text condition, we employ Gemini to infer the textual description, the adaptive location box of the visual condition, and the visual condition-related box through a three-stage reasoning process. The input visual condition is then affixed to a predefined background, serving as the seed initialization for the cycle. During the Cyclic One-Way Diffusion process, we ``disturb'' and ``reconstruct'' the image in a cyclic way and ensure a continuous one-way diffusion by consistently replacing it with corresponding $\mathbf{x_t}$. Also, by integrating these prior pieces of information, we execute cyclic diffusion with dynamic attention modulation, enhancing the coherence and accuracy of the generated outputs.
                      </p>
                  </figcaption>
                  <!-- <br>
                  <p>
                  <ul>
                      <li>
                          <b>Multimodal Encoding Stage.</b> Leveraging existing well-established models to encode inputs of various modalities.
                          Here we take advantage of the ImageBind, which is a unified high-performance encoder across six modalities.
                          Then, via the linear projection layer, different input representations are mapped into language-like representations that
                          are comprehensible to the LLM.
                      </li>
                      <li>
                          <b>LLM Understanding and Reasoning Stage.</b>
                          An LLM is used as the core agent of NExT-GPT.
                          Technically, we employ the Vicuna.
                          LLM takes as input the representations from different modalities and carries out semantic understanding and reasoning over
                          the inputs.
                          It outputs 1) the textual responses directly, and 2) signal tokens of each modality that serve as instructions to dictate
                          the decoding layers whether to generate multimodal contents, and what content to produce if yes.
                      </li>
                      <li>
                          <b>Multimodal Generation Stage.</b>
                          Receiving the multimodal signals with specific instructions from LLM (if any), the Transformer-based output projection
                          layers map the signal token representations into the ones that are understandable to following multimodal decoders.
                          Technically, we employ the current off-the-shelf latent conditioned diffusion models of different modal generations, i.e.,
                          Stable Diffusion (SD) for image synthesis, Zeroscope for video synthesis, and AudioLDM for audio synthesis.
                      </li>
                  </ul>
                  </p>
                  <br> -->
<!-- 
                  <img class="columns is-centered has-text-centered" src="./static/images/config.png" alt="Teaser" width="85%"
                       style="margin:0 auto"> -->
              </div>
              <br/>

          </div>
      </div>
      <!--/ Architecture -->
    </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
      <div class="columns is-centered">
          <div class="column is-8">
              <div class="content">
                  <p style="text-align: center;">
                      The webpage is built based on <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                  </p>
              </div>
          </div>
      </div>
  </div>
</footer>

</body>
</html>
