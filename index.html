<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SOWing information: Cultivating Contextual Coherence with MLLMs in Image Generation">
  <meta name="keywords" content="Diffusion models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SOWing information: Cultivating Contextual Coherence with MLLMs in Image Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SOWing information: Cultivating Contextual Coherence with MLLMs in Image Generation.</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Yuhan Pei*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Ruoyu Wang*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Yongqi Yang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Ye Zhu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Olga Russakovsky</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Yu Wu</a><sup>1</sup>,
            </span>
            <!-- <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Wuhan University</span>
            <span class="author-block"><sup>2</sup>Princeton University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
 -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Originating from the diffusion phenomenon in physics, which describes the random movement and collisions of particles, diffusion generative models simulate a random walk in the data space along the denoising trajectory. 
            This allows information to diffuse across regions, yielding harmonious outcomes.
            However, the chaotic and disordered nature of information diffusion in diffusion models often results in undesired interference between image regions, causing degraded detail preservation and contextual inconsistency.
            In this work, we address these challenges by reframing disordered diffusion as a powerful tool for text-vision-to-image generation (TV2I) tasks, achieving pixel-level condition fidelity while maintaining visual and semantic coherence throughout the image. 
            We first introduce Cyclic One-Way Diffusion (COW), which provides an efficient unidirectional diffusion framework for precise information transfer while minimizing disruptive interference. 
            Building on COW, we further propose Selective One-Way Diffusion (SOW), which utilizes Multimodal Large Language Models (MLLMs) to clarify the semantic and spatial relationships within the image. Based on these insights, SOW combines attention mechanisms to dynamically regulate the direction and intensity of diffusion according to contextual relationships.
            Extensive experiments demonstrate the untapped potential of controlled information diffusion, offering a path to more adaptive and versatile generative models in a learning-free manner. 
          </p>
        </div>
        <div class="content has-text-justified">
          <img class="columns is-centered has-text-centered" src="./static/images/compare_newa1_00.png" alt="Teaser" width="100%"
               style="margin:0 auto">
          <br>
          <figcaption>
              <p style="text-align: left;">
                  <font color="061E61">
                      <b>Figure 1:</b> Comparison with existing SOTA methods for maintaining the fidelity of text and visual conditions in different application scenarios. We consistently achieve superior fidelity to both text and visual conditions in all three settings. In contrast, other learning-based approaches struggle to attain the same level of performance in diverse scenarios. 
                  </font>
              </p>
          </figcaption>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Background</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion in physics is a phenomenon caused by random movements and collisions between particles.
            The diffusion model, drawing inspiration from non-equilibrium thermodynamics, establishes a Markov chain between a target data distribution and the Gaussian distribution.
            Illustration of ``diffusion in diffusion''. In experiment (a), We invert the pictures of pure gray and white to \( \mathbf{x_t} \), merge them together, and then regenerate them to \( \mathbf{x_0} \) via deterministic denoising. In experiment (b), we enhance the attention scores of the upper right quartile to the lower left quartile, while in experiment (c) we suppress attention scores from the upper right quartile towards other areas. The resulting images show how regions within an image diffuse and interfere with each other during denoising, and reveal the direct effect of attention on diffusion.  
          </p>
        </div>
        <div class="content has-text-justified">
          <img class="columns is-centered has-text-centered" src="./static/images/diffusionindiffusion.png" alt="Teaser" width="100%"
               style="margin:0 auto">
          <br>
          <!-- <figcaption>
              <p style="text-align: left;">
                  <font color="061E61">
                      <b>Figure 2:</b> 
                  </font>
              </p>
          </figcaption> -->
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
          <h2 class="title is-2">Method</h2>
          <br>
      </div>

      <!-- Architecture -->
      <div class="columns is-centered">
          <div class="column is-full-width">
              <!-- <h4 class="title is-3">• Framework</h4> -->
              <div class="content has-text-justified">
                <!-- <p style="text-align: left; color: #061E61;"> -->
                  <p>
                  The pipeline of our proposed SOW method. Initially, given the visual condition and text condition, we employ Gemini to infer the textual description, the adaptive location box of the visual condition, and the visual condition-related box through a three-stage reasoning process. The input visual condition is then affixed to a predefined background, serving as the seed initialization for the cycle. During the Cyclic One-Way Diffusion process, we ``disturb'' and ``reconstruct'' the image in a cyclic way and ensure a continuous one-way diffusion by consistently replacing it with corresponding \(\mathbf{x_t}\). Also, by integrating these prior pieces of information, we execute cyclic diffusion with dynamic attention modulation, enhancing the coherence and accuracy of the generated outputs.
                  </p>
                  <img class="columns is-centered has-text-centered" src="./static/images/pipeline_new2-1.png" alt="Teaser" width="95%"
                       style="margin:0 auto">
                  <br>
                  <figcaption>

                  </figcaption>
                  <!-- <br>
                  <p>
                  <ul>
                      <li>
                          <b>Multimodal Encoding Stage.</b> Leveraging existing well-established models to encode inputs of various modalities.
                          Here we take advantage of the ImageBind, which is a unified high-performance encoder across six modalities.
                          Then, via the linear projection layer, different input representations are mapped into language-like representations that
                          are comprehensible to the LLM.
                      </li>
                      <li>
                          <b>LLM Understanding and Reasoning Stage.</b>
                          An LLM is used as the core agent of NExT-GPT.
                          Technically, we employ the Vicuna.
                          LLM takes as input the representations from different modalities and carries out semantic understanding and reasoning over
                          the inputs.
                          It outputs 1) the textual responses directly, and 2) signal tokens of each modality that serve as instructions to dictate
                          the decoding layers whether to generate multimodal contents, and what content to produce if yes.
                      </li>
                      <li>
                          <b>Multimodal Generation Stage.</b>
                          Receiving the multimodal signals with specific instructions from LLM (if any), the Transformer-based output projection
                          layers map the signal token representations into the ones that are understandable to following multimodal decoders.
                          Technically, we employ the current off-the-shelf latent conditioned diffusion models of different modal generations, i.e.,
                          Stable Diffusion (SD) for image synthesis, Zeroscope for video synthesis, and AudioLDM for audio synthesis.
                      </li>
                  </ul>
                  </p>
                  <br> -->
<!-- 
                  <img class="columns is-centered has-text-centered" src="./static/images/config.png" alt="Teaser" width="85%"
                       style="margin:0 auto"> -->
              </div>
              <br/>

          </div>
      </div>
      <!--/ Architecture -->
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <h4 class="title is-4">• Tradeoffs between Text and Visual Conditions</h4>
          <p>
            SOW strikes a balance between meeting the visual and text conditions. For example, when given a photo of a young woman but the text is “an \textit{old} person”, our method can make the woman older to meet the text description by adding wrinkles, changing skin elasticity and hair color, etc. while maintaining the facial expression and the identity of the given woman.
          </p>
          <img class="columns is-centered has-text-centered" src="./static/images/conflict_new-1.png" alt="Teaser" width="100%"
          style="margin:0 auto">
          <br>
          <h4 class="title is-4">• Generalized Visual Condition</h4>
        </div>
        <div class="content has-text-justified">
          <p>SOW can be directly applied to other visual conditions other than human faces.</p>
          <img class="columns is-centered has-text-centered" src="./static/images/other_obj_new2-1.png" alt="Teaser" width="100%"
               style="margin:0 auto">
          <br>
          <!-- <figcaption>
              <p style="text-align: left;">
                  <font color="061E61">
                      <b>Figure 1:</b> Comparison with existing SOTA methods for maintaining the fidelity of text and visual conditions in different application scenarios. We consistently achieve superior fidelity to both text and visual conditions in all three settings. In contrast, other learning-based approaches struggle to attain the same level of performance in diverse scenarios. 
                  </font>
              </p>
          </figcaption> -->
      </div>
    </div>
  </div>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
      <div class="columns is-centered">
          <div class="column is-8">
              <div class="content">
                  <p style="text-align: center;">
                      The webpage is built based on <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                  </p>
              </div>
          </div>
      </div>
  </div>
</footer>

</body>
</html>
